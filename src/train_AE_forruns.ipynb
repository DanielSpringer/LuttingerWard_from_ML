{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT6jGCyeda1U",
    "outputId": "7deec4f1-feea-4917-d6b4-14d65e2928de"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install gdown\n",
    "# import gdown\n",
    "# %pwd\n",
    "# %ls\n",
    "# %cd /content\n",
    "# %cd drive\n",
    "# %cd MyDrive\n",
    "# %mkdir -p Satellite\n",
    "# %cd Satellite\n",
    "# !nvidia-smi\n",
    "# url = 'https://drive.google.com/file/d/1hCM96mRJeYiivFFtQJcTK6w6nYh8GJ-Q'\n",
    "# gdown.download_folder(url)\n",
    "# !pip install pytorch_lightning\n",
    "# !pip update pytorch_lightning\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('drive/MyDrive/LuttingerWard_Prediction/')\n",
    "# # import data\n",
    "# # # import drive.MyDrive.LuttingerWard_Prediction.data\n",
    "\n",
    "# !ls drive/MyDrive/LuttingerWard_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import torch\n",
    "import models\n",
    "import wrappers\n",
    "from torch.utils.data import DataLoader\n",
    "import load_data\n",
    "import datetime\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.plugins.environments import LightningEnvironment\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XtZLONWvRvIg"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/data/U2c0_b10b50_gmax_10x10.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     train()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### > Separate training and validation HDF5 files \u001b[39;00m\n\u001b[1;32m     11\u001b[0m ld \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDATA_LOADER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m234\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# validation_set = getattr(ld, config[\"DATA_LOADER\"])(config, data_type = \"valid\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/code/load_data.py:161\u001b[0m, in \u001b[0;36mDataset_convergence_split.__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m### Using noise to produce a gradient\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# PATH = config[\"PATH_TRAIN\"]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# f = h5py.File(PATH, 'r')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m### Using more validation samples with half of them the actual target to produce a gradient\u001b[39;00m\n\u001b[1;32m    160\u001b[0m PATH \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH_TRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m data_in \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    163\u001b[0m g0 \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/gpfs/opt/sw/jupyterhub/envs/conda/vsc5/jupyterhub-matlab/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/gpfs/opt/sw/jupyterhub/envs/conda/vsc5/jupyterhub-matlab/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/data/U2c0_b10b50_gmax_10x10.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    ### JSON File contains full information about entire run (model, data, hyperparameters)\n",
    "    ### TODO \n",
    "    MODEL_NAME = \"CONVERGENCE_AUTO_ENCODER_1\"\n",
    "    config = json.load(open('confmod_auto_encoder.json'))[MODEL_NAME]\n",
    "    # MODEL_NAME = \"AUTO_ENCODER_1\"\n",
    "    # config = json.load(open('confmod_auto_encoder.json'))[MODEL_NAME]\n",
    "\n",
    "    ''' Dataloading '''\n",
    "    ### > Separate training and validation HDF5 files \n",
    "    ld = __import__(\"load_data\")\n",
    "    train_set = getattr(ld, config[\"DATA_LOADER\"])(config, data_type = \"train\", target_sample = 234)\n",
    "    # validation_set = getattr(ld, config[\"DATA_LOADER\"])(config, data_type = \"valid\")\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    # validation_dataloader = DataLoader(validation_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    ### > Single HDF5 file containing training and validation data \n",
    "    # data_set = load_data.Dataset_ae(config)\n",
    "    # # train_set, validation_set, unused_set = torch.utils.data.random_split(data_set, [int(data_set.__len__()*0.3), int(data_set.__len__()*0.05), int(data_set.__len__()*0.65)], generator=torch.Generator().manual_seed(42))\n",
    "    # train_set, validation_set = torch.utils.data.random_split(data_set, [int(data_set.__len__()*0.8), int(data_set.__len__()*0.2)], generator=torch.Generator().manual_seed(42))\n",
    "    # train_dataloader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    # validation_dataloader = DataLoader(validation_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "\n",
    "    ''' Model setup '''\n",
    "    wrapers = __import__(\"wrapers\")\n",
    "    model = getattr(wrapers, config[\"MODEL_WRAPER\"])(config)\n",
    "\n",
    "\n",
    "    ''' Model loading from save file '''\n",
    "    if config[\"continue\"] == True:\n",
    "        SAVEPATH = config[\"SAVEPATH\"]\n",
    "        checkpoint = torch.load(SAVEPATH)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\" >>> Loaded checkpoint\")\n",
    "\n",
    "\n",
    "    ''' Logging and saving '''\n",
    "    DATA_NAME = os.path.splitext(os.path.basename(config[\"PATH_TRAIN\"]))[0]\n",
    "\n",
    "    PATH = \"\"\n",
    "    CONFIGURATION = f\"../saves/{DATA_NAME}/save_{config['MODEL_NAME']}_BS{config['batch_size']}_{datetime.datetime.now().date()}\"\n",
    "    # CONFIGURATION = f\"../saves/save_{config['MODEL_NAME']}_Nodes{config['n_nodes']}_BS{config['batch_size']}_{datetime.datetime.now().date()}\"\n",
    "    logger = TensorBoardLogger(PATH, name=CONFIGURATION)\n",
    "\n",
    "\n",
    "    # '''Define (pytorch_lightning) Trainer '''\n",
    "    # ### > SLURM Training\n",
    "    # trainer = pl.Trainer(max_epochs=config[\"epochs\"], accelerator=config[\"device_type\"], devices=config[\"devices\"], num_nodes=config[\"num_nodes\"], strategy='ddp', logger=logger)\n",
    "    # ### > Jupyter Notebook Training\n",
    "    # trainer = pl.Trainer(max_epochs=20, accelerator='gpu', devices=1, strategy='auto', logger=logger, plugins=[LightningEnvironment()])\n",
    "    # ### > Jupyter Notebook CPU Training\n",
    "    trainer = pl.Trainer(max_epochs=500, accelerator='cpu', devices=1, strategy='auto', logger=logger, plugins=[LightningEnvironment()])\n",
    "    \n",
    "    ''' Train '''\n",
    "    # trainer.fit(model, train_dataloader, validation_dataloader)\n",
    "    trainer.fit(model, train_dataloader)\n",
    "\n",
    "    ''' Saving configuration file into log folder ''' \n",
    "    LOGDIR = trainer.log_dir\n",
    "    json_object = json.dumps(config, indent=4)\n",
    "    with open(LOGDIR+\"/config.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
