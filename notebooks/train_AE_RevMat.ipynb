{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import torch\n",
    "import models\n",
    "# import wrapers\n",
    "from torch.utils.data import DataLoader\n",
    "# import load_data\n",
    "import datetime\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.plugins.environments import LightningEnvironment\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XtZLONWvRvIg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/code/load_data.py:166: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data_in = torch.tensor(data_in)#[None] # BATCH DIMENSION\n",
      "/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/code/load_data.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data_target = torch.cat([torch.tensor(data_in.real, dtype=torch.float32), torch.tensor(data_in.imag, dtype=torch.float32)], axis=1)\n",
      "/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/code/load_data.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data_in = torch.cat([torch.tensor(data_in.real, dtype=torch.float32), torch.tensor(data_in.imag, dtype=torch.float32)], axis=1)\n",
      "/gpfs/data/fs72150/springerd/Projects/LuttingerWard_from_ML/code/load_data.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.g0 = torch.cat([torch.tensor(g0.real, dtype=torch.float32), torch.tensor(g0.imag, dtype=torch.float32)], axis=1)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | encoder_model   | model_wraper_encgiv      | 307 K \n",
      "1 | static_LW_model | model_wraper_G0injection | 838 K \n",
      "2 | criterion_mse   | MSELoss                  | 0     \n",
      "-------------------------------------------------------------\n",
      "307 K     Trainable params\n",
      "838 K     Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.584     Total estimated model params size (MB)\n",
      "/home/fs72150/springerd/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/fs72150/springerd/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21ea1fffd55466bbb1e68bd321223aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    ### JSON File contains full information about entire run (model, data, hyperparameters)\n",
    "    ### TODO \n",
    "    MODEL_NAME = \"CONVERGENCE_AUTO_ENCODER_1\"\n",
    "    config = json.load(open('confmod_auto_encoder.json'))[MODEL_NAME]\n",
    "    # MODEL_NAME = \"AUTO_ENCODER_1\"\n",
    "    # config = json.load(open('confmod_auto_encoder.json'))[MODEL_NAME]\n",
    "\n",
    "    ''' Dataloading '''\n",
    "    ### > Separate training and validation HDF5 files \n",
    "    rand_samples = torch.randint(0,1000,(100,), dtype=int)\n",
    "    sample_idxs = torch.ones(100, dtype=int) * 941\n",
    "    sample_idx = torch.cat([sample_idxs, rand_samples], axis=0)\n",
    "    sample_idx, _ = torch.sort(sample_idx)\n",
    "\n",
    "    ld = __import__(\"load_data\")\n",
    "    train_set = getattr(ld, config[\"DATA_LOADER\"])(config, data_type = \"valid\", target_sample = sample_idx)\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "#     validation_set = getattr(ld, config[\"DATA_LOADER\"])(config, data_type = \"valid\")\n",
    "#     validation_dataloader = DataLoader(validation_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    ### > Single HDF5 file containing training and validation data \n",
    "    # data_set = load_data.Dataset_ae(config)\n",
    "    # # train_set, validation_set, unused_set = torch.utils.data.random_split(data_set, [int(data_set.__len__()*0.3), int(data_set.__len__()*0.05), int(data_set.__len__()*0.65)], generator=torch.Generator().manual_seed(42))\n",
    "    # train_set, validation_set = torch.utils.data.random_split(data_set, [int(data_set.__len__()*0.8), int(data_set.__len__()*0.2)], generator=torch.Generator().manual_seed(42))\n",
    "    # train_dataloader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    # validation_dataloader = DataLoader(validation_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "\n",
    "    ''' Model setup '''\n",
    "    wrapers = __import__(\"wrapers\")\n",
    "    model = getattr(wrapers, config[\"MODEL_WRAPER\"])(config)\n",
    "\n",
    "\n",
    "    ''' Model loading from save file '''\n",
    "    if config[\"continue\"] == True:\n",
    "        SAVEPATH = config[\"SAVEPATH\"]\n",
    "        checkpoint = torch.load(SAVEPATH)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\" >>> Loaded checkpoint\")\n",
    "\n",
    "\n",
    "    ''' Logging and saving '''\n",
    "    DATA_NAME = os.path.splitext(os.path.basename(config[\"PATH_TRAIN\"]))[0]\n",
    "\n",
    "    PATH = \"\"\n",
    "    CONFIGURATION = f\"../saves/{DATA_NAME}/save_{config['MODEL_NAME']}_BS{config['batch_size']}_{datetime.datetime.now().date()}\"\n",
    "    # CONFIGURATION = f\"../saves/save_{config['MODEL_NAME']}_Nodes{config['n_nodes']}_BS{config['batch_size']}_{datetime.datetime.now().date()}\"\n",
    "    logger = TensorBoardLogger(PATH, name=CONFIGURATION)\n",
    "\n",
    "\n",
    "    # '''Define (pytorch_lightning) Trainer '''\n",
    "    # ### > SLURM Training\n",
    "#     trainer = pl.Trainer(max_epochs=config[\"epochs\"], accelerator=config[\"device_type\"], devices=config[\"devices\"], num_nodes=config[\"num_nodes\"], strategy='ddp', logger=logger)\n",
    "    # ### > Jupyter Notebook Training\n",
    "    # trainer = pl.Trainer(max_epochs=5000, accelerator='gpu', devices=1, strategy='auto', logger=logger, plugins=[LightningEnvironment()])\n",
    "    # ### > Jupyter Notebook CPU Training\n",
    "    trainer = pl.Trainer(max_epochs=2000, accelerator='cpu', devices=1, strategy='auto', logger=logger, plugins=[LightningEnvironment()])\n",
    "    \n",
    "    ''' Train '''\n",
    "#     trainer.fit(model, train_dataloader, validation_dataloader)\n",
    "    trainer.fit(model, train_dataloader)\n",
    "\n",
    "    ''' Saving configuration file into log folder ''' \n",
    "    LOGDIR = trainer.log_dir\n",
    "    json_object = json.dumps(config, indent=4)\n",
    "    with open(LOGDIR+\"/config.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3)\n",
      "[[[0.15095104 0.91295823 0.94432776]\n",
      "  [0.54219063 0.69414251 0.18925607]\n",
      "  [0.88253536 0.75250278 0.20890615]]\n",
      "\n",
      " [[0.08338908 0.89899906 0.79755765]\n",
      "  [0.76296112 0.21669249 0.68776967]\n",
      "  [0.76097889 0.10513168 0.18541401]]\n",
      "\n",
      " [[0.34356213 0.37234785 0.18116331]\n",
      "  [0.45682426 0.16098292 0.68639817]\n",
      "  [0.78312005 0.00743915 0.81195064]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.zeros((3,3,3))\n",
    "print(x.shape)\n",
    "print(np.random.rand(*x.shape))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
